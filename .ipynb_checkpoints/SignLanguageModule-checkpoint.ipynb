{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2720c7",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab420fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOTE:\n",
    "###### This notebook has to be converted into a python (.py) file\n",
    "###### when imported in other notebooks in the same folder\n",
    "###### in order for its functions to be accessible by the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.5.0 opencv-python mediapipe sklearn matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyttsx3 # speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abc0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import string\n",
    "\n",
    "# Sound\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455131d4",
   "metadata": {},
   "source": [
    "# 2. Setup Folders for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# Signs to detect\n",
    "signs = np.array(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                   'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "                   'hello', 'my', 'name', 'is', 'nice', 'you', 'bye', 'thank you', 'to meet'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 60 #30\n",
    "\n",
    "# Each video has 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 60 #30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary folders for the signs defined previously\n",
    "for sign in signs:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, sign, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdc872",
   "metadata": {},
   "source": [
    "# 3. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a126a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and leveraging the model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# For easier drawing of keypoints on face\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157da674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make detection\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd196195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate extracted keypoints values into numpy arrays\n",
    "# If no detection is made on any of the following: pose/face/left hand/right hand, arrays are filled with zeros\n",
    "# calculation of arrays zero: mediapipe landmarks * number of coordinates values (eg: x, y, z, visibility)\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8055e",
   "metadata": {},
   "source": [
    "# 4. Speech Conversion for Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get factory reference to pyttsx3.Engine instance\n",
    "engine = pyttsx3.init()\n",
    "# Get current value of voices for the engine property\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voices', voices[0].id)\n",
    "# Change speech rate with 150 words per minute \n",
    "engine.setProperty('rate', 150)\n",
    "\n",
    "# Function to output voice\n",
    "def speak(str):\n",
    "    engine.say(str)\n",
    "    engine.runAndWait()\n",
    "    engine.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081b591",
   "metadata": {},
   "source": [
    "# 5. Collecting Keypoints Values for Training/Testing Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abff377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is invoked when collecting keypoints values for signs' sequences (videos)\n",
    "# during training/testing data collection\n",
    "def collect_keypoints():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        # Loop through signs\n",
    "        for sign in signs: \n",
    "            # Loop through sequences (videos)\n",
    "            for sequence in range(no_sequences):\n",
    "                # Loop through video length (sequence length)\n",
    "                for frame_num in range(sequence_length):\n",
    "                    \n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "                    \n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "                    \n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "                    \n",
    "                    # Apply wait logic\n",
    "                    if frame_num == 0:\n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)     \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(sign, sequence), (15, 12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('Image Collection for Model', image)\n",
    "                        cv2.waitKey(2000) # 2 seconds break interval to reposition before collecting next sequence(video)\n",
    "                        \n",
    "                    else:\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(sign, sequence), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)       \n",
    "                        # Show to screen\n",
    "                        cv2.imshow('Image Collection for Model', image)\n",
    "                    \n",
    "                    # Export Keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, sign, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "                    \n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1d605",
   "metadata": {},
   "source": [
    "# 6. Build the LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building LSTM neural network\n",
    "from tensorflow.keras.models import Sequential # build sequential neural network\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For monitoring neural network accuracy while training through the web app\n",
    "\n",
    "# To access:\n",
    "# 1. Open cmd, go to the folder (ConvertingSignLanguageToSpeech\\Logs\\train)\n",
    "# 2. Enter and run the command \"tensorboard --logdir=.\" to open TensorBoard from within the current folder\n",
    "# 3. Copy the link given into a web browser to monitor the neural network while training\n",
    "# or, \n",
    "# 1. Open Anaconda Prompt, activate the virtual environment of the project where Tensorflow is \n",
    "# (e.g: conda activate sign_language_to_speech)\n",
    "# 2. Enter and run the command \"tensorboard --logdir=PATH_TO_LOG_FILES\"\n",
    "# (e.g: C:\\Users\\Dylan\\ConvertingSignLanguageToSpeech\\Logs\\train)\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa30a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to instantiate the model\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662))) # change the value\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(700, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(700, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, activation='relu')) \n",
    "    model.add(Dense(signs.shape[0], activation='softmax'))\n",
    "\n",
    "    # Specify the loss for multi-class classification model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c686869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a list of 26 alphabets\n",
    "alphabets_list = list(string.ascii_lowercase)\n",
    "\n",
    "print(alphabets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0991f",
   "metadata": {},
   "source": [
    "# 7. Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e400b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is invoked when performing the real-time detection of converting sign language to speech\n",
    "# in the SignLanguageToSpeech notebook, after loading the trained LSTM neural network\n",
    "def sign_to_speech():\n",
    "    # New detection variables\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    predictions = [] \n",
    "    threshold = 0.8 #0.8\n",
    "    pTime = 0\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            cTime = time.time()\n",
    "            fps = 1/ (cTime - pTime)\n",
    "            pTime = cTime\n",
    "            cv2.putText(image, f'FPS: {int(fps)}', (150, 70), cv2.FONT_HERSHEY_PLAIN,\n",
    "                       3, (0, 255, 0), 2)\n",
    "            \n",
    "            \n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "\n",
    "            if len(sequence) == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(signs[np.argmax(res)])\n",
    "                predictions.append(np.argmax(res))\n",
    "                print(res[np.argmax(res)])\n",
    "\n",
    "            # Viz logic\n",
    "                # Check to make sure last 10 frames have exact same prediction for stability in prediction\n",
    "                if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                    if res[np.argmax(res)] >= threshold:\n",
    "                        if len(sentence) > 0:\n",
    "                            \n",
    "                            # Check if predicted sign gestures returns an alphabet\n",
    "#                             if len(signs[np.argmax(res)]) == 1: \n",
    "                            if signs[np.argmax(res)] in alphabets_list:\n",
    "                                \n",
    "                                # Two conditions when appending predicted sign gestures to sentence: \n",
    "                                # 1. Check if recent word in sentence is an alphabet\n",
    "                                # 2. Check if recent word in sentence is not in signs array\n",
    "                                # Concatenate the recent word with the predicted alphabet in sentence\n",
    "#                                 if len(sentence[-1]) >= 1 or sentence[-1] not in signs:\n",
    "                                if sentence[-1] in alphabets_list or sentence[-1] not in signs:\n",
    "                                    speak(signs[np.argmax(res)])\n",
    "                                    sentence[-1] = sentence[-1]+signs[np.argmax(res)]\n",
    "                            \n",
    "                            # Check if predicted sign gestures returns non-alphabets\n",
    "                            else:\n",
    "                                # Append it to sentence and output to speech\n",
    "#                                 sentence.append(signs[np.argmax(res)])\n",
    "#                                 speak(sentence[-1])\n",
    "                                if signs[np.argmax(res)] != sentence[-1]:\n",
    "                                    sentence.append(signs[np.argmax(res)])\n",
    "                                    speak(sentence[-1])\n",
    "                                \n",
    "                        else:\n",
    "                            sentence.append(signs[np.argmax(res)])\n",
    "                            speak(sentence[-1])\n",
    "                        \n",
    "                \n",
    "                # Take last five values to prevent rendering a giant sentence array\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (3, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('Sign Language Detection to Speech', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e21e8",
   "metadata": {},
   "source": [
    "# 8. Testing Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is just to test the webcam is accessed for making detections\n",
    "def test_camera():\n",
    "    \n",
    "    cap = cv2.VideoCapture(0) # Change the value to 1 if using external webcam\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('Test Webcam', frame)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
